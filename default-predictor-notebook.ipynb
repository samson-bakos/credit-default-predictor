{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b9d092-c86e-4f74-a7b3-e48d727e9c6b",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction\n",
    "\n",
    "The purpose of this notebook it to construct a full machine learning pipeline to predict the probability of a customer defaulting on their credit card debt. The assignment prompt for this is adapted from an assignment in UBC MDS DSCI 572, but has been reworked. The data is sourced from the Kaggle Credit Card Clients Dataset, found here https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4773c-e8de-4d99-bf4b-1fc938fb467d",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "The problem is a classification task to identify customers who will default on their credit card payments in the next month. In the raw data set, this column is labeled as `default.payment.next.month`, but this will be re-labeled and referred to to as `default` for convenience. A 0 indicates no default, and a 1 indicates a default. There is significant class imbalance in the dataset, with the majority being non default. As such the analysis will be primarily concerned with metrics related to the identifying the positive class (default), and this can be considered an anomaly detection problem, meaning raw accuracy is an unreliable metric. \n",
    "\n",
    "Initial model evaluation will be based on maximal f1 score. This analysis will consider a balanced approach, as we want to avoid casting an overly broad net and having excessive false positives to ensure that intervention as specifically targeted as possible. However, we can consider minimizing false negatives (recall) to be more damaging due to profit loss to the credit card company, while intervention against an individual who would not default may be still warranted if the model identifies them as sufficiently high risk. Overall, we want a somewhat balanced model that still prioritizes minimizing type II error (false negative) over type I error (false postive).\n",
    "\n",
    "The ideal method would be to set an minimum acceptable operating point for precision, and use this to determine a probability threshold that maximizes recall for each model. The model with the highest recall at this operating point is the model selected by this modeling process. However, setting this operating point requires additional industry knowledge, as well as being specific to the risk tolerance of the company/ deployment context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f628b6-1d89-4853-bb76-08230dd53e59",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46bcd38-1d7c-44ee-a8a7-dcefce448ab6",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "1. Approximately 22% of examples are default (positive class), 78% are not.\n",
    "2. We have 24 potential features and one target\n",
    "\n",
    "Feature Descriptions:\n",
    "\n",
    "* `ID`: Unique identifier, will be dropped\n",
    "* `LIMIT_BAL`: Maximum credit, numeric\n",
    "* `SEX` : Binary/ Numeric. We don't necessarily want to make decisions on default prediction based on sex for ethical reasons, so this will be dropped.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
